{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Research Assistant","text":"<p>A RAG (Retrieval-Augmented Generation) system for exploring academic papers from arXiv. Built to quickly find and query specific papers across the vast AI literature\u2014helping keep track of the fast-moving state of the art.</p>"},{"location":"#highlights","title":"Highlights","text":"Feature Description Paper Ingestion Automated collection from arXiv with HTML parsing for better text extraction Vector Search Semantic search with configurable embedding models and Qdrant vector database Two-Stage Retrieval Cross-encoder reranking for precision on top of fast vector search Production API FastAPI backend with streaming responses and SvelteKit frontend"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automated Paper Collection: Fetch papers from arXiv using configurable search queries</li> <li>Smart Text Extraction: Use ar5iv HTML for cleaner text than raw PDFs</li> <li>Configurable Embeddings: Swap embedding models without code changes</li> <li>Two-Stage Retrieval: Vector search + cross-encoder reranking</li> <li>Streaming Responses: Real-time answer generation with SSE</li> <li>Modern Frontend: SvelteKit UI with dark mode and citation support</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Clone and install\ngit clone https://github.com/agapestack/research-assistant\ncd research-assistant\nuv sync\n\n# Start Qdrant\ndocker compose up -d\n\n# Collect and index papers\nuv run python scripts/run_flow.py collect --days 30 --max-per-query 20\nuv run python scripts/run_flow.py index --limit 20\n\n# Start the API\nuv run uvicorn src.main:app --reload\n\n# Start the frontend (separate terminal)\ncd frontend &amp;&amp; npm install &amp;&amp; npm run dev\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart LR\n    subgraph Ingestion\n        A[arXiv API] --&gt; B[ar5iv HTML]\n        B --&gt; C[Chunking]\n    end\n\n    subgraph Storage\n        C --&gt; D[Embeddings]\n        D --&gt; E[(Qdrant)]\n    end\n\n    subgraph Retrieval\n        F[Query] --&gt; G[Vector Search]\n        G --&gt; E\n        E --&gt; H[Reranker]\n        H --&gt; I[LLM]\n    end\n\n    I --&gt; J[Answer + Citations]</code></pre>"},{"location":"#project-goals","title":"Project Goals","text":"<p>This project was built to demonstrate:</p> <ol> <li>End-to-end RAG implementation from data ingestion to user interface</li> <li>Production-quality code with proper architecture and testing</li> <li>Engineering decision-making with documented trade-offs</li> <li>SOTA model selection using MTEB leaderboard</li> <li>Modern Python tooling (uv, FastAPI, Pydantic, type hints)</li> </ol>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>Explore the documentation to understand:</p> <ul> <li>How papers are ingested and processed</li> <li>Chunking strategies for academic text</li> <li>Embedding model selection</li> <li>Two-stage retrieval with reranking</li> <li>System architecture decisions</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete documentation of the REST API endpoints.</p>"},{"location":"api/#base-url","title":"Base URL","text":"<pre><code>http://localhost:8000\n</code></pre>"},{"location":"api/#endpoints","title":"Endpoints","text":""},{"location":"api/#query","title":"Query","text":""},{"location":"api/#post-query","title":"<code>POST /query</code>","text":"<p>Query the RAG system with a natural language question.</p> <p>Request:</p> <pre><code>{\n  \"question\": \"What is retrieval augmented generation?\",\n  \"k\": 5,\n  \"model\": \"qwen3:14b\"\n}\n</code></pre> Field Type Default Description <code>question</code> string required The question to ask <code>k</code> integer 5 Number of sources to retrieve <code>model</code> string null LLM model (null = default) <p>Response:</p> <pre><code>{\n  \"answer\": \"Retrieval-Augmented Generation (RAG) is a technique that combines...[1]...[2]\",\n  \"sources\": [\n    {\n      \"id\": 1,\n      \"title\": \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n      \"arxiv_url\": \"https://arxiv.org/abs/2005.11401\",\n      \"authors\": \"Lewis et al.\",\n      \"page\": 2,\n      \"content\": \"We explore a general-purpose fine-tuning recipe...\",\n      \"score\": 0.8234,\n      \"original_score\": 0.7123\n    }\n  ],\n  \"model\": \"qwen3:14b\"\n}\n</code></pre>"},{"location":"api/#post-querystream","title":"<code>POST /query/stream</code>","text":"<p>Stream the response using Server-Sent Events.</p> <p>Request: Same as <code>/query</code></p> <p>Response: SSE stream</p> <pre><code>data: {\"type\": \"sources\", \"sources\": [...], \"model\": \"qwen3:14b\"}\n\ndata: {\"type\": \"chunk\", \"content\": \"Retrieval\"}\ndata: {\"type\": \"chunk\", \"content\": \"-Augmented\"}\ndata: {\"type\": \"chunk\", \"content\": \" Generation\"}\n...\ndata: {\"type\": \"done\"}\n</code></pre> <p>JavaScript Example:</p> <pre><code>const response = await fetch('/query/stream', {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify({ question: \"What is RAG?\" })\n});\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n\n  const text = decoder.decode(value);\n  const lines = text.split('\\n');\n\n  for (const line of lines) {\n    if (line.startsWith('data: ')) {\n      const data = JSON.parse(line.slice(6));\n      if (data.type === 'chunk') {\n        console.log(data.content);\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"api/#search","title":"Search","text":""},{"location":"api/#post-search","title":"<code>POST /search</code>","text":"<p>Semantic search without LLM generation.</p> <p>Request:</p> <pre><code>{\n  \"query\": \"transformer attention mechanism\",\n  \"k\": 10\n}\n</code></pre> <p>Response:</p> <pre><code>[\n  {\n    \"content\": \"The Transformer architecture relies entirely on self-attention...\",\n    \"title\": \"Attention Is All You Need\",\n    \"arxiv_url\": \"https://arxiv.org/abs/1706.03762\",\n    \"page\": 3,\n    \"score\": 0.8456\n  }\n]\n</code></pre>"},{"location":"api/#index","title":"Index","text":""},{"location":"api/#post-index","title":"<code>POST /index</code>","text":"<p>Fetch papers from arXiv and index them.</p> <p>Request:</p> <pre><code>{\n  \"topic\": \"retrieval augmented generation\",\n  \"max_results\": 10\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"message\": \"Indexed papers on 'retrieval augmented generation'\",\n  \"papers_found\": 10,\n  \"papers_indexed\": 8,\n  \"documents_indexed\": 423\n}\n</code></pre> <p>Note</p> <p><code>papers_indexed</code> may be less than <code>papers_found</code> if some papers don't have ar5iv HTML available.</p>"},{"location":"api/#follow-ups","title":"Follow-ups","text":""},{"location":"api/#post-followups","title":"<code>POST /followups</code>","text":"<p>Generate follow-up question suggestions.</p> <p>Request:</p> <pre><code>{\n  \"question\": \"What is RAG?\",\n  \"answer\": \"RAG combines retrieval with generation...\"\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"questions\": [\n    \"How does RAG compare to fine-tuning?\",\n    \"What are the limitations of RAG?\",\n    \"Which vector databases work best with RAG?\"\n  ]\n}\n</code></pre>"},{"location":"api/#models","title":"Models","text":""},{"location":"api/#get-models","title":"<code>GET /models</code>","text":"<p>List available LLM models.</p> <p>Response:</p> <pre><code>{\n  \"default\": \"qwen3:14b\",\n  \"available\": {\n    \"qwen3:14b\": \"Qwen3 14B - Best balance for RAG\",\n    \"gemma3:12b\": \"Gemma 3 12B - Best for chat\",\n    \"mistral-small:24b\": \"Mistral Small 24B - High quality\",\n    \"mistral-nemo\": \"Mistral Nemo 12B - Fast inference\"\n  }\n}\n</code></pre>"},{"location":"api/#stats","title":"Stats","text":""},{"location":"api/#get-stats","title":"<code>GET /stats</code>","text":"<p>Get vector store statistics.</p> <p>Response:</p> <pre><code>{\n  \"total_documents\": 45230,\n  \"collection_name\": \"papers_qwen3-4b\"\n}\n</code></pre>"},{"location":"api/#health","title":"Health","text":""},{"location":"api/#get-health","title":"<code>GET /health</code>","text":"<p>Health check endpoint.</p> <p>Response:</p> <pre><code>{\n  \"status\": \"ok\"\n}\n</code></pre>"},{"location":"api/#error-responses","title":"Error Responses","text":""},{"location":"api/#400-bad-request","title":"400 Bad Request","text":"<pre><code>{\n  \"detail\": \"Unknown model. Available: ['qwen3:14b', 'gemma3:12b', ...]\"\n}\n</code></pre>"},{"location":"api/#422-validation-error","title":"422 Validation Error","text":"<pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\"body\", \"question\"],\n      \"msg\": \"field required\",\n      \"type\": \"value_error.missing\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api/#500-internal-server-error","title":"500 Internal Server Error","text":"<pre><code>{\n  \"detail\": \"Connection to Qdrant failed\"\n}\n</code></pre>"},{"location":"api/#rate-limiting","title":"Rate Limiting","text":"<p>No rate limiting is implemented. For production, consider:</p> <ul> <li>API key authentication</li> <li>Request rate limiting</li> <li>Response caching</li> </ul>"},{"location":"api/#cors","title":"CORS","text":"<p>Allowed origins:</p> <pre><code>allow_origins=[\"http://localhost:5173\", \"http://127.0.0.1:5173\"]\n</code></pre> <p>Modify in <code>src/main.py</code> for production.</p>"},{"location":"api/#openapi-documentation","title":"OpenAPI Documentation","text":"<p>Interactive docs available at:</p> <ul> <li>Swagger UI: http://localhost:8000/docs</li> <li>ReDoc: http://localhost:8000/redoc</li> <li>OpenAPI JSON: http://localhost:8000/openapi.json</li> </ul>"},{"location":"deployment/","title":"Deployment","text":"<p>How to deploy the Research Assistant in various environments.</p>"},{"location":"deployment/#local-development","title":"Local Development","text":""},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>Docker (for Qdrant)</li> <li>Node.js 18+ (for frontend)</li> <li>Ollama (for LLM)</li> </ul>"},{"location":"deployment/#quick-start","title":"Quick Start","text":"<pre><code># 1. Clone and install\ngit clone https://github.com/agapestack/research-assistant\ncd research-assistant\nuv sync\n\n# 2. Start Qdrant\ndocker compose up -d qdrant\n\n# 3. Pull an LLM model\nollama pull qwen3:14b\n\n# 4. Index some papers (via API or scripts)\n\n# 5. Start the API\nuv run uvicorn src.main:app --reload --host 0.0.0.0 --port 8000\n\n# 6. Start the frontend (new terminal)\ncd frontend\nnpm install\nnpm run dev\n</code></pre>"},{"location":"deployment/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file:</p> <pre><code># Qdrant\nRAG_QDRANT_HOST=localhost\nRAG_QDRANT_PORT=6333\n\n# Embeddings\nRAG_EMBEDDING_MODEL=qwen3-4b\n\n# LLM\nRAG_LLM_MODEL=qwen3:14b\nRAG_LLM_TEMPERATURE=0.1\n\n# Retrieval\nRAG_RETRIEVAL_K=5\n</code></pre>"},{"location":"deployment/#testing","title":"Testing","text":"<pre><code># Unit tests (mocked, no GPU required)\nuv run python -m pytest tests/ --ignore=tests/test_integration.py -v\n\n# Integration tests (requires GPU + Qdrant)\nuv run python -m pytest tests/test_integration.py -v\n\n# Linting\nuv run ruff check src/ tests/\n</code></pre>"},{"location":"deployment/#docker-compose-full-stack","title":"Docker Compose (Full Stack)","text":""},{"location":"deployment/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>services:\n  qdrant:\n    image: qdrant/qdrant:latest\n    ports:\n      - \"6333:6333\"\n      - \"6334:6334\"\n    volumes:\n      - qdrant_data:/qdrant/storage\n\n  api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - RAG_QDRANT_HOST=qdrant\n      - RAG_EMBEDDING_MODEL=qwen3-4b\n    depends_on:\n      - qdrant\n\n  frontend:\n    build: ./frontend\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - api\n\nvolumes:\n  qdrant_data:\n</code></pre>"},{"location":"deployment/#dockerfile-api","title":"Dockerfile (API)","text":"<pre><code>FROM python:3.12-slim\n\nWORKDIR /app\n\n# Install uv\nRUN pip install uv\n\n# Copy and install dependencies\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --no-dev\n\n# Copy source code\nCOPY src/ ./src/\n\n# Run the API\nCMD [\"uv\", \"run\", \"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"deployment/#build-and-run","title":"Build and Run","text":"<pre><code>docker compose up -d\n</code></pre>"},{"location":"deployment/#production-considerations","title":"Production Considerations","text":""},{"location":"deployment/#1-reverse-proxy-nginx","title":"1. Reverse Proxy (Nginx)","text":"<pre><code>server {\n    listen 80;\n    server_name api.yourdomain.com;\n\n    location / {\n        proxy_pass http://localhost:8000;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_cache_bypass $http_upgrade;\n    }\n}\n</code></pre>"},{"location":"deployment/#2-ssl-with-certbot","title":"2. SSL with Certbot","text":"<pre><code>sudo certbot --nginx -d api.yourdomain.com\n</code></pre>"},{"location":"deployment/#3-process-manager-systemd","title":"3. Process Manager (systemd)","text":"<pre><code># /etc/systemd/system/research-assistant.service\n[Unit]\nDescription=Research Assistant API\nAfter=network.target\n\n[Service]\nUser=ubuntu\nWorkingDirectory=/home/ubuntu/research-assistant\nExecStart=/home/ubuntu/.local/bin/uv run uvicorn src.main:app --host 0.0.0.0 --port 8000\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <pre><code>sudo systemctl enable research-assistant\nsudo systemctl start research-assistant\n</code></pre>"},{"location":"deployment/#4-environment-security","title":"4. Environment Security","text":"<pre><code># Never commit .env files\necho \".env\" &gt;&gt; .gitignore\n\n# Use secrets manager in production\n# AWS: Secrets Manager\n# GCP: Secret Manager\n# Azure: Key Vault\n</code></pre>"},{"location":"deployment/#cloud-deployment","title":"Cloud Deployment","text":""},{"location":"deployment/#aws-ec2-docker","title":"AWS (EC2 + Docker)","text":"<pre><code># 1. Launch EC2 instance (t3.medium or larger)\n# 2. Install Docker\nsudo yum install docker -y\nsudo systemctl start docker\n\n# 3. Install docker-compose\nsudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\n\n# 4. Clone and run\ngit clone https://github.com/agapestack/research-assistant\ncd research-assistant\ndocker-compose up -d\n</code></pre>"},{"location":"deployment/#gcp-cloud-run","title":"GCP (Cloud Run)","text":"<pre><code># Build and push image\ngcloud builds submit --tag gcr.io/PROJECT_ID/research-assistant\n\n# Deploy\ngcloud run deploy research-assistant \\\n  --image gcr.io/PROJECT_ID/research-assistant \\\n  --platform managed \\\n  --region us-central1 \\\n  --allow-unauthenticated\n</code></pre>"},{"location":"deployment/#railway-render","title":"Railway / Render","text":"<p>Both support direct GitHub deployment:</p> <ol> <li>Connect your repository</li> <li>Set environment variables</li> <li>Deploy automatically on push</li> </ol>"},{"location":"deployment/#monitoring","title":"Monitoring","text":""},{"location":"deployment/#health-checks","title":"Health Checks","text":"<pre><code># API health\ncurl http://localhost:8000/health\n\n# Qdrant health\ncurl http://localhost:6333/health\n</code></pre>"},{"location":"deployment/#logging","title":"Logging","text":"<p>Configure structured logging:</p> <pre><code>import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n</code></pre>"},{"location":"deployment/#metrics-future","title":"Metrics (Future)","text":"<p>Consider adding:</p> <ul> <li>Prometheus metrics endpoint</li> <li>Request latency histograms</li> <li>Error rate tracking</li> <li>Vector store size monitoring</li> </ul>"},{"location":"deployment/#scaling","title":"Scaling","text":""},{"location":"deployment/#horizontal-scaling","title":"Horizontal Scaling","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Load        \u2502\n                    \u2502 Balancer    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502               \u2502               \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   API #1    \u2502 \u2502   API #2    \u2502 \u2502   API #3    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502               \u2502               \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Qdrant    \u2502\n                    \u2502  (Cluster)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/#qdrant-cluster","title":"Qdrant Cluster","text":"<p>For high availability:</p> <pre><code># docker-compose.yml\nservices:\n  qdrant-node-1:\n    image: qdrant/qdrant:latest\n    environment:\n      - QDRANT__CLUSTER__ENABLED=true\n    # ...\n\n  qdrant-node-2:\n    image: qdrant/qdrant:latest\n    environment:\n      - QDRANT__CLUSTER__ENABLED=true\n    # ...\n</code></pre>"},{"location":"deployment/#github-pages-documentation","title":"GitHub Pages (Documentation)","text":"<p>This documentation site is deployed via GitHub Actions:</p> <pre><code># .github/workflows/docs.yml\nname: Deploy Documentation\n\non:\n  push:\n    branches: [main]\n    paths: ['docs/**', 'mkdocs.yml']\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n      - run: pip install mkdocs-material mkdocs-mermaid2-plugin\n      - run: mkdocs gh-deploy --force\n</code></pre> <p>Access at: <code>https://agapestack.github.io/research-assistant</code></p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>This document explains the system architecture, design decisions, and how components interact.</p>"},{"location":"architecture/overview/#system-diagram","title":"System Diagram","text":"<pre><code>flowchart TB\n    subgraph Client\n        UI[SvelteKit Frontend]\n    end\n\n    subgraph API[\"FastAPI Backend\"]\n        direction TB\n        EP[Endpoints]\n        RC[RAG Chain]\n        VS[Vector Store]\n        RR[Reranker]\n    end\n\n    subgraph Storage\n        QD[(Qdrant)]\n        SQL[(SQLite)]\n    end\n\n    subgraph External\n        ARX[arXiv API]\n        AR5[ar5iv HTML]\n        OLL[Ollama LLM]\n    end\n\n    subgraph Workflows[\"Prefect Workflows\"]\n        COL[Collection Flow]\n        IDX[Indexing Flow]\n    end\n\n    UI &lt;--&gt;|REST/SSE| EP\n    EP --&gt; RC\n    RC --&gt; VS\n    RC --&gt; RR\n    RC --&gt; OLL\n    VS &lt;--&gt; QD\n\n    COL --&gt; ARX\n    COL --&gt; SQL\n    IDX --&gt; AR5\n    IDX --&gt; VS\n    IDX --&gt; SQL</code></pre>"},{"location":"architecture/overview/#component-responsibilities","title":"Component Responsibilities","text":""},{"location":"architecture/overview/#api-layer-srcmainpy","title":"API Layer (<code>src/main.py</code>)","text":"Endpoint Method Purpose <code>/query</code> POST RAG query with sources <code>/query/stream</code> POST Streaming response via SSE <code>/search</code> POST Vector search only (no LLM) <code>/index</code> POST Manual paper indexing <code>/models</code> GET List available LLM models <code>/stats</code> GET Vector store statistics <code>/health</code> GET Health check"},{"location":"architecture/overview/#services-layer-srcservices","title":"Services Layer (<code>src/services/</code>)","text":"<pre><code>services/\n\u251c\u2500\u2500 embeddings.py      # Embedding model abstraction\n\u251c\u2500\u2500 vector_store.py    # Qdrant client wrapper\n\u251c\u2500\u2500 reranker.py        # Cross-encoder reranking\n\u251c\u2500\u2500 rag_chain.py       # RAG orchestration\n\u251c\u2500\u2500 document_loader.py # PDF/HTML \u2192 chunks\n\u251c\u2500\u2500 html_fetcher.py    # ar5iv HTML parsing\n\u2514\u2500\u2500 arxiv_fetcher.py   # arXiv API client\n</code></pre>"},{"location":"architecture/overview/#workflow-layer-srcworkflows","title":"Workflow Layer (<code>src/workflows/</code>)","text":"<p>Prefect flows for automated paper collection and indexing:</p> <ul> <li>Collection Flow: Search arXiv \u2192 deduplicate \u2192 save to SQLite</li> <li>Indexing Flow: Fetch HTML \u2192 chunk \u2192 embed \u2192 store in Qdrant</li> <li>Orchestrator: Combines collection + indexing</li> </ul>"},{"location":"architecture/overview/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/overview/#why-ar5iv-html-over-pdfs","title":"Why ar5iv HTML over PDFs?","text":"Aspect PDF ar5iv HTML Text extraction Noisy, layout issues Clean, structured Section boundaries Hard to detect Explicit <code>&lt;section&gt;</code> tags Math rendering Often corrupted Proper LaTeX Availability All papers ~80% of papers <p>Decision: Use ar5iv as primary source, fall back gracefully when unavailable.</p>"},{"location":"architecture/overview/#why-qdrant-over-alternatives","title":"Why Qdrant over alternatives?","text":"Database Pros Cons Qdrant Fast, filtering, easy setup Less ecosystem ChromaDB Simple, embedded Performance at scale Pinecone Managed, scalable Cost, vendor lock-in Weaviate Feature-rich Complex setup <p>Decision: Qdrant provides the best balance of performance, features, and simplicity for a self-hosted solution.</p>"},{"location":"architecture/overview/#why-two-stage-retrieval","title":"Why Two-Stage Retrieval?","text":"<pre><code>flowchart LR\n    Q[Query] --&gt; VS[Vector Search&lt;br/&gt;k=20, fast]\n    VS --&gt; RR[Reranker&lt;br/&gt;top_k=5, precise]\n    RR --&gt; LLM[LLM Generation]</code></pre> <ol> <li>Stage 1 - Vector Search: Fast approximate search over entire corpus (k=20)</li> <li>Stage 2 - Cross-Encoder: Precise reranking of candidates (top_k=5)</li> </ol> <p>This approach gets the best of both worlds: - Speed of embedding-based search - Precision of cross-encoder scoring</p>"},{"location":"architecture/overview/#why-sentence-transformers-over-openai","title":"Why Sentence Transformers over OpenAI?","text":"Aspect Sentence Transformers OpenAI Embeddings Cost Free (local) $0.0001/1K tokens Latency ~10ms ~100-500ms Privacy Data stays local Sent to API Customization Many models Limited options <p>Decision: Local embeddings for cost, speed, and privacy. OpenAI can be added as an option later.</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#ingestion-pipeline","title":"Ingestion Pipeline","text":"<pre><code>sequenceDiagram\n    participant A as arXiv API\n    participant S as SQLite\n    participant H as ar5iv\n    participant C as Chunker\n    participant E as Embedder\n    participant Q as Qdrant\n\n    Note over A,Q: Collection Phase\n    A-&gt;&gt;S: Search results (metadata)\n    S-&gt;&gt;S: Deduplicate &amp; store\n\n    Note over A,Q: Indexing Phase\n    S-&gt;&gt;H: Get unindexed papers\n    H-&gt;&gt;C: HTML content\n    C-&gt;&gt;E: Text chunks\n    E-&gt;&gt;Q: Vectors + metadata\n    Q-&gt;&gt;S: Mark as indexed</code></pre>"},{"location":"architecture/overview/#query-pipeline","title":"Query Pipeline","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant API as FastAPI\n    participant VS as VectorStore\n    participant RR as Reranker\n    participant LLM as Ollama\n\n    U-&gt;&gt;API: Question\n    API-&gt;&gt;VS: Embed &amp; search (k=20)\n    VS-&gt;&gt;API: Candidate chunks\n    API-&gt;&gt;RR: Rerank candidates\n    RR-&gt;&gt;API: Top 5 chunks\n    API-&gt;&gt;LLM: Context + question\n    LLM--&gt;&gt;API: Streamed answer\n    API--&gt;&gt;U: SSE chunks</code></pre>"},{"location":"architecture/overview/#scalability-considerations","title":"Scalability Considerations","text":""},{"location":"architecture/overview/#current-limitations","title":"Current Limitations","text":"<ul> <li>Single-node Qdrant (can be clustered)</li> <li>Synchronous embedding (can be batched)</li> <li>No caching layer (Redis can be added)</li> </ul>"},{"location":"architecture/overview/#scaling-path","title":"Scaling Path","text":"<ol> <li>More papers: Qdrant handles millions of vectors</li> <li>More users: Add Redis caching, load balancing</li> <li>Better quality: Upgrade embedding model, add hybrid search</li> <li>Production: Kubernetes deployment, monitoring</li> </ol>"},{"location":"architecture/tech-stack/","title":"Technology Stack","text":"<p>A detailed breakdown of every technology used and why it was chosen.</p>"},{"location":"architecture/tech-stack/#core-technologies","title":"Core Technologies","text":""},{"location":"architecture/tech-stack/#python-312","title":"Python 3.12+","text":"<p>The entire backend is Python, chosen for:</p> <ul> <li>Rich ML/AI ecosystem</li> <li>FastAPI async support</li> <li>Type hints for maintainability</li> <li>Extensive libraries for NLP</li> </ul>"},{"location":"architecture/tech-stack/#uv-package-manager","title":"uv Package Manager","text":"<pre><code># Instead of pip/poetry\nuv sync        # Install dependencies\nuv add package # Add new package\nuv run script  # Run with venv\n</code></pre> <p>Why uv?</p> <ul> <li>10-100x faster than pip</li> <li>Built-in virtual environment management</li> <li>Lock file for reproducibility</li> <li>Written in Rust</li> </ul>"},{"location":"architecture/tech-stack/#backend-framework","title":"Backend Framework","text":""},{"location":"architecture/tech-stack/#fastapi","title":"FastAPI","text":"<pre><code>@app.post(\"/query\", response_model=QueryResponse)\nasync def query(request: QueryRequest):\n    result = await rag.aquery(request.question, k=request.k)\n    return QueryResponse(**result)\n</code></pre> <p>Why FastAPI?</p> Feature Benefit Async native Handle concurrent requests Pydantic models Automatic validation OpenAPI docs Auto-generated <code>/docs</code> Type hints IDE support, fewer bugs"},{"location":"architecture/tech-stack/#pydantic-settings","title":"Pydantic Settings","text":"<pre><code>class Settings(BaseSettings):\n    model_config = SettingsConfigDict(env_file=\".env\", env_prefix=\"RAG_\")\n\n    qdrant_host: str = \"localhost\"\n    embedding_model: str = \"qwen3-4b\"\n    llm_model: str = \"qwen3:14b\"\n</code></pre> <p>Configuration via environment variables with type safety.</p>"},{"location":"architecture/tech-stack/#llm-orchestration","title":"LLM Orchestration","text":""},{"location":"architecture/tech-stack/#langchain","title":"LangChain","text":"<p>Used for:</p> <ul> <li>Prompt templates</li> <li>Output parsing</li> <li>Chain composition</li> </ul> <pre><code>from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([...])\nchain = prompt | llm | StrOutputParser()\n</code></pre> <p>Why LangChain?</p> <ul> <li>Standard abstractions</li> <li>Easy LLM swapping</li> <li>Built-in streaming support</li> </ul>"},{"location":"architecture/tech-stack/#ollama","title":"Ollama","text":"<p>Local LLM inference:</p> <pre><code>ollama pull qwen3:14b\n</code></pre> <p>Available Models:</p> Model Size Use Case qwen3:14b 14B Best RAG balance gemma3:12b 12B Good for chat mistral-small:24b 24B High quality mistral-nemo 12B Fast inference"},{"location":"architecture/tech-stack/#vector-database","title":"Vector Database","text":""},{"location":"architecture/tech-stack/#qdrant","title":"Qdrant","text":"<pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\n\nclient = QdrantClient(host=\"localhost\", port=6333)\nclient.create_collection(\n    collection_name=\"papers_bge-base\",\n    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n)\n</code></pre> <p>Why Qdrant?</p> <ul> <li>Written in Rust (fast)</li> <li>Filtering support</li> <li>Easy Docker deployment</li> <li>Good Python client</li> <li>Free and open source</li> </ul>"},{"location":"architecture/tech-stack/#collection-strategy","title":"Collection Strategy","text":"<p>Each embedding model gets its own collection:</p> <pre><code>papers_qwen3-4b    (2560 dims)\npapers_qwen3-0.6b  (1024 dims)\npapers_bge-large   (1024 dims)\n</code></pre> <p>This allows A/B testing different models.</p>"},{"location":"architecture/tech-stack/#embeddings","title":"Embeddings","text":""},{"location":"architecture/tech-stack/#sentence-transformers","title":"Sentence Transformers","text":"<pre><code>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-4B\")\nvectors = model.encode([\"text1\", \"text2\"])\n</code></pre> <p>Models selected based on MTEB Leaderboard.</p> Model Dimensions MTEB Retrieval Qwen3-Embedding-8B 4096 70.58 Qwen3-Embedding-4B 4096 ~68 Qwen3-Embedding-0.6B 4096 ~65"},{"location":"architecture/tech-stack/#reranker","title":"Reranker","text":"<pre><code>from transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"jinaai/jina-reranker-v3\", trust_remote_code=True)\nresults = model.rerank(query, documents)\n</code></pre> <p>Selected based on MTEB Reranking leaderboard.</p>"},{"location":"architecture/tech-stack/#workflow-orchestration","title":"Workflow Orchestration","text":""},{"location":"architecture/tech-stack/#prefect","title":"Prefect","text":"<pre><code>from prefect import flow, task\n\n@task(retries=3, retry_delay_seconds=30)\ndef search_arxiv(query: str, max_results: int):\n    ...\n\n@flow(name=\"collect-papers\")\ndef collect_papers_flow(days_back: int = 30):\n    results = search_arxiv(query, max_results)\n    ...\n</code></pre> <p>Why Prefect?</p> <ul> <li>Python-native workflows</li> <li>Automatic retries</li> <li>Caching support</li> <li>Nice UI for monitoring</li> </ul>"},{"location":"architecture/tech-stack/#frontend","title":"Frontend","text":""},{"location":"architecture/tech-stack/#sveltekit-5","title":"SvelteKit 5","text":"<pre><code>&lt;script&gt;\n  let answer = '';\n\n  async function query() {\n    const response = await fetch('/api/query', {...});\n    for await (const chunk of streamResponse(response)) {\n      answer += chunk;\n    }\n  }\n&lt;/script&gt;\n</code></pre> <p>Why SvelteKit?</p> <ul> <li>Minimal bundle size</li> <li>Great DX</li> <li>Built-in SSR</li> <li>Reactive by default</li> </ul>"},{"location":"architecture/tech-stack/#tailwind-css","title":"Tailwind CSS","text":"<p>Utility-first styling with dark mode support.</p>"},{"location":"architecture/tech-stack/#data-storage","title":"Data Storage","text":""},{"location":"architecture/tech-stack/#sqlite","title":"SQLite","text":"<p>Tracking paper metadata and indexing status:</p> <pre><code>CREATE TABLE papers (\n    arxiv_id TEXT PRIMARY KEY,\n    title TEXT,\n    authors TEXT,\n    is_indexed INTEGER DEFAULT 0,\n    html_available INTEGER\n);\n</code></pre> <p>Why SQLite?</p> <ul> <li>Zero configuration</li> <li>Single file</li> <li>Good enough for metadata</li> <li>Easy to backup</li> </ul>"},{"location":"architecture/tech-stack/#document-processing","title":"Document Processing","text":""},{"location":"architecture/tech-stack/#pymupdf-fitz","title":"PyMuPDF (fitz)","text":"<p>PDF text extraction:</p> <pre><code>import fitz\ndoc = fitz.open(\"paper.pdf\")\ntext = doc[0].get_text()\n</code></pre>"},{"location":"architecture/tech-stack/#beautifulsoup","title":"BeautifulSoup","text":"<p>HTML parsing for ar5iv:</p> <pre><code>from bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, \"html.parser\")\nsections = soup.find_all(\"section\")\n</code></pre>"},{"location":"architecture/tech-stack/#langchain-text-splitters","title":"LangChain Text Splitters","text":"<pre><code>from langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n)\n</code></pre>"},{"location":"architecture/tech-stack/#development-tools","title":"Development Tools","text":"Tool Purpose pytest Testing ruff Linting &amp; formatting"},{"location":"architecture/tech-stack/#infrastructure","title":"Infrastructure","text":""},{"location":"architecture/tech-stack/#docker-compose","title":"Docker Compose","text":"<pre><code>services:\n  qdrant:\n    image: qdrant/qdrant:latest\n    ports:\n      - \"6333:6333\"\n    volumes:\n      - qdrant_data:/qdrant/storage\n</code></pre>"},{"location":"architecture/tech-stack/#github-actions","title":"GitHub Actions","text":"<p>CI/CD for:</p> <ul> <li>Linting (ruff)</li> <li>Building documentation</li> <li>Deploying to GitHub Pages</li> </ul>"},{"location":"pipeline/chunking/","title":"Chunking Strategy","text":"<p>How documents are split into chunks for effective retrieval.</p>"},{"location":"pipeline/chunking/#why-chunking-matters","title":"Why Chunking Matters","text":"<p>LLMs have context limits, and embedding models work best on focused text segments. Chunking determines:</p> <ul> <li>Retrieval precision: Smaller chunks = more precise matches</li> <li>Context completeness: Larger chunks = more context per result</li> <li>Storage efficiency: Chunk size affects vector count</li> </ul>"},{"location":"pipeline/chunking/#the-trade-off","title":"The Trade-off","text":"<pre><code>graph LR\n    A[Small Chunks&lt;br/&gt;256-512 chars] --&gt;|More precise| B[Better matching]\n    A --&gt;|Less context| C[Incomplete answers]\n\n    D[Large Chunks&lt;br/&gt;1500-2000 chars] --&gt;|More context| E[Complete information]\n    D --&gt;|Less precise| F[Irrelevant content]\n\n    G[Medium Chunks&lt;br/&gt;800-1200 chars] --&gt;|Balanced| H[Good for most cases]</code></pre>"},{"location":"pipeline/chunking/#our-configuration","title":"Our Configuration","text":"<pre><code>from langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n)\n</code></pre>"},{"location":"pipeline/chunking/#parameters-explained","title":"Parameters Explained","text":"Parameter Value Reasoning <code>chunk_size</code> 1000 ~150-200 tokens, fits in context <code>chunk_overlap</code> 200 Preserves context at boundaries <code>separators</code> Hierarchical Respects document structure"},{"location":"pipeline/chunking/#separator-hierarchy","title":"Separator Hierarchy","text":"<p>The splitter tries separators in order:</p> <ol> <li><code>\\n\\n</code> - Paragraph breaks (best)</li> <li><code>\\n</code> - Line breaks</li> <li><code>.</code> - Sentence boundaries</li> <li><code></code> - Word boundaries</li> <li>`` - Character level (last resort)</li> </ol>"},{"location":"pipeline/chunking/#implementation","title":"Implementation","text":"<pre><code>def chunk_document(\n    pages: list[dict],\n    source: str,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 200,\n    extra_metadata: dict | None = None,\n) -&gt; list[Document]:\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n    )\n\n    documents = []\n    for page_data in pages:\n        chunks = splitter.split_text(page_data[\"text\"])\n        for i, chunk in enumerate(chunks):\n            metadata = {\n                \"source\": source,\n                \"page\": page_data[\"page\"],  # Section number for HTML\n                \"chunk\": i,\n            }\n            if extra_metadata:\n                metadata.update(extra_metadata)\n            documents.append(Document(page_content=chunk, metadata=metadata))\n\n    return documents\n</code></pre>"},{"location":"pipeline/chunking/#metadata-preservation","title":"Metadata Preservation","text":"<p>Each chunk carries metadata for citation:</p> <pre><code>{\n    \"source\": \"ar5iv:2401.12345\",\n    \"page\": 3,  # Section index\n    \"chunk\": 0,  # Chunk within section\n    \"title\": \"Attention Is All You Need\",\n    \"authors\": \"Vaswani et al.\",\n    \"arxiv_url\": \"https://arxiv.org/abs/2401.12345\",\n    \"arxiv_id\": \"2401.12345\",\n    \"published\": \"2024-01-15T00:00:00\",\n}\n</code></pre>"},{"location":"pipeline/chunking/#section-aware-chunking","title":"Section-Aware Chunking","text":"<p>For ar5iv HTML, we treat each section as a \"page\":</p> <pre><code>def extract_text_from_html(paper_html: PaperHTML) -&gt; list[dict]:\n    pages = []\n\n    # Abstract as page 0\n    if paper_html.abstract:\n        pages.append({\"text\": f\"Abstract\\n\\n{paper_html.abstract}\", \"page\": 0})\n\n    # Each section as a page\n    for i, section in enumerate(paper_html.sections, start=1):\n        text = f\"{section.title}\\n\\n{section.content}\"\n        pages.append({\"text\": text, \"page\": i})\n\n    return pages\n</code></pre> <p>This preserves document structure and enables section-level filtering.</p>"},{"location":"pipeline/chunking/#chunk-size-analysis","title":"Chunk Size Analysis","text":""},{"location":"pipeline/chunking/#academic-paper-statistics","title":"Academic Paper Statistics","text":"<p>Typical paper after chunking:</p> Paper Length Sections Chunks (1000 char) Short (4 pages) 5-6 20-30 Medium (8 pages) 8-10 40-60 Long (15+ pages) 12-15 80-120"},{"location":"pipeline/chunking/#storage-impact","title":"Storage Impact","text":"<pre><code>Storage per chunk \u2248 2 KB\n  - Vector (768 dims): 768 \u00d7 4 bytes = 3 KB\n  - Metadata: ~500 bytes\n  - Content: ~1 KB\n\n1000 papers \u00d7 50 chunks \u00d7 2 KB = 100 MB\n</code></pre>"},{"location":"pipeline/chunking/#alternative-strategies","title":"Alternative Strategies","text":""},{"location":"pipeline/chunking/#semantic-chunking","title":"Semantic Chunking","text":"<p>Split by meaning rather than character count:</p> <pre><code># Not implemented, but possible approach\nfrom langchain_experimental.text_splitter import SemanticChunker\n\nchunker = SemanticChunker(\n    embeddings=embeddings,\n    breakpoint_threshold_type=\"percentile\"\n)\n</code></pre> <p>Pros: Better semantic boundaries Cons: Slower, inconsistent chunk sizes</p>"},{"location":"pipeline/chunking/#sentence-level-chunking","title":"Sentence-Level Chunking","text":"<pre><code>import nltk\nsentences = nltk.sent_tokenize(text)\n# Group sentences into ~1000 char chunks\n</code></pre> <p>Pros: Clean boundaries Cons: May split related content</p>"},{"location":"pipeline/chunking/#sliding-window","title":"Sliding Window","text":"<pre><code># Fixed window with overlap\nwindow_size = 1000\nstep = 800  # 200 char overlap\n\nchunks = [text[i:i+window_size] for i in range(0, len(text), step)]\n</code></pre> <p>Pros: Simple, predictable Cons: Ignores structure</p>"},{"location":"pipeline/chunking/#recommendations-by-use-case","title":"Recommendations by Use Case","text":"Use Case Chunk Size Overlap Notes Q&amp;A 800-1000 200 Balance precision/context Summarization 1500-2000 300 More context needed Semantic Search 500-800 100 Precision matters Chat/Conversation 1000-1200 200 Good for follow-ups"},{"location":"pipeline/chunking/#experimentation","title":"Experimentation","text":"<p>You can experiment with different settings:</p> <pre><code># In document_loader.py or via API\ndocs = load_paper_from_html(\n    arxiv_id=\"2401.12345\",\n    metadata={...},\n    chunk_size=800,    # Try different values\n    chunk_overlap=150,\n)\n</code></pre> <p>Monitor retrieval quality with:</p> <pre><code>uv run python scripts/evaluate_retrieval.py --compare\n</code></pre>"},{"location":"pipeline/embeddings/","title":"Embedding Models","text":"<p>How text is converted to vectors for semantic search.</p>"},{"location":"pipeline/embeddings/#what-are-embeddings","title":"What Are Embeddings?","text":"<p>Embeddings convert text into dense numerical vectors that capture semantic meaning:</p> <pre><code>\"What is machine learning?\" \u2192 [0.12, -0.34, 0.56, ..., 0.78]  (4096 dimensions)\n</code></pre> <p>Similar texts have similar vectors (high cosine similarity).</p>"},{"location":"pipeline/embeddings/#model-selection","title":"Model Selection","text":"<p>Models are selected based on the MTEB Leaderboard retrieval scores. We use Qwen3-Embedding as the default, which ranks #1 on MTEB multilingual retrieval (as of June 2025).</p>"},{"location":"pipeline/embeddings/#our-abstraction-layer","title":"Our Abstraction Layer","text":"<p>We built a configurable embedding system:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass Embedder(ABC):\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Model identifier for collection naming.\"\"\"\n\n    @property\n    @abstractmethod\n    def dimensions(self) -&gt; int:\n        \"\"\"Vector dimensions.\"\"\"\n\n    @abstractmethod\n    def encode(self, texts: str | list[str]) -&gt; list[list[float]]:\n        \"\"\"Encode text(s) to vectors.\"\"\"\n\n    @abstractmethod\n    def encode_query(self, query: str) -&gt; list[float]:\n        \"\"\"Encode a query (may use different prompts/prefixes).\"\"\"\n</code></pre>"},{"location":"pipeline/embeddings/#switching-models","title":"Switching Models","text":"<p>Change via environment variable:</p> <pre><code># In .env\nRAG_EMBEDDING_MODEL=qwen3-4b\n</code></pre> <p>Or pass directly:</p> <pre><code>vs = VectorStore(embedding_model=\"qwen3-4b\")\n</code></pre> <p>Each model creates its own collection: <code>papers_qwen3-0.6b</code>, <code>papers_qwen3-4b</code>, etc.</p>"},{"location":"pipeline/embeddings/#available-models","title":"Available Models","text":"<pre><code>MODELS = {\n    # Qwen3 SOTA (requires transformers&gt;=4.51.0, sentence-transformers&gt;=2.7.0)\n    \"qwen3-8b\": \"Qwen/Qwen3-Embedding-8B\",\n    \"qwen3-4b\": \"Qwen/Qwen3-Embedding-4B\",\n    \"qwen3-0.6b\": \"Qwen/Qwen3-Embedding-0.6B\",\n\n    # Fast &amp; small\n    \"minilm\": \"all-MiniLM-L6-v2\",\n\n    # BGE family\n    \"bge-small\": \"BAAI/bge-small-en-v1.5\",\n    \"bge-base\": \"BAAI/bge-base-en-v1.5\",\n    \"bge-large\": \"BAAI/bge-large-en-v1.5\",\n\n    # E5 family\n    \"e5-small\": \"intfloat/e5-small-v2\",\n    \"e5-base\": \"intfloat/e5-base-v2\",\n    \"e5-large\": \"intfloat/e5-large-v2\",\n\n    # GTE\n    \"gte-small\": \"thenlper/gte-small\",\n    \"gte-base\": \"thenlper/gte-base\",\n    \"gte-large\": \"thenlper/gte-large\",\n\n    # Scientific text\n    \"specter2\": \"allenai/specter2_base\",\n}\n</code></pre>"},{"location":"pipeline/embeddings/#model-comparison","title":"Model Comparison","text":"Model Dimensions Parameters MTEB Retrieval VRAM qwen3-8b 4096 8B 70.58 ~16 GB qwen3-4b 2560 4B ~68 ~10 GB qwen3-0.6b 1024 0.6B ~65 ~2-3 GB bge-large 1024 335M 54.29 ~4 GB bge-base 768 110M 53.25 ~1 GB minilm 384 23M ~42 ~500 MB <p>Recommendation</p> <p>Use qwen3-4b as the default for excellent quality with reasonable VRAM (~10GB). Use qwen3-0.6b for lower resource environments.</p>"},{"location":"pipeline/embeddings/#qwen3-model-features","title":"Qwen3 Model Features","text":"<ul> <li>Multilingual: 100+ languages including code</li> <li>Long context: Up to 32K tokens</li> <li>Flash Attention 2: Enabled by default for memory efficiency</li> <li>Instruction-aware: Uses task-specific prompts for queries</li> </ul>"},{"location":"pipeline/embeddings/#special-query-handling","title":"Special Query Handling","text":"<p>Qwen3 models use the <code>prompt_name=\"query\"</code> parameter for queries:</p> <pre><code>def encode_query(self, query: str) -&gt; list[float]:\n    if self._name in self._QWEN3_MODELS:\n        return self._model.encode(query, prompt_name=\"query\").tolist()\n    return self._model.encode(query).tolist()\n</code></pre>"},{"location":"pipeline/embeddings/#e5-model-special-handling","title":"E5 Model Special Handling","text":"<p>E5 models require prefixes:</p> <pre><code>def encode(self, texts: str | list[str]) -&gt; list[list[float]]:\n    if self._name in self._E5_MODELS:\n        texts = [f\"passage: {t}\" for t in texts]\n    return self._model.encode(texts).tolist()\n\ndef encode_query(self, query: str) -&gt; list[float]:\n    if self._name in self._E5_MODELS:\n        return self._model.encode(f\"query: {query}\").tolist()\n    return self._model.encode(query).tolist()\n</code></pre>"},{"location":"pipeline/embeddings/#how-to-choose","title":"How to Choose","text":"<p>Models selected based on MTEB Leaderboard retrieval scores.</p> Use Case Model Prototyping minilm Production qwen3-4b Maximum quality qwen3-8b Low VRAM qwen3-0.6b"},{"location":"pipeline/embeddings/#storage-considerations","title":"Storage Considerations","text":"Model Dims Bytes/Vector 100K Docs minilm 384 1.5 KB 150 MB bge-base 768 3 KB 300 MB qwen3-0.6b 1024 4 KB 400 MB qwen3-4b 2560 10 KB 1 GB <p>Qdrant handles millions of vectors efficiently with HNSW indexing.</p>"},{"location":"pipeline/ingestion/","title":"Paper Ingestion Pipeline","text":"<p>How academic papers are collected from arXiv and prepared for indexing.</p>"},{"location":"pipeline/ingestion/#overview","title":"Overview","text":"<pre><code>flowchart LR\n    A[arXiv API] --&gt;|Search| B[Paper Metadata]\n    B --&gt;|Store| C[(SQLite)]\n    C --&gt;|Fetch| D[ar5iv HTML]\n    D --&gt;|Parse| E[Sections]\n    E --&gt;|Chunk| F[Documents]\n    F --&gt;|Embed| G[(Qdrant)]</code></pre>"},{"location":"pipeline/ingestion/#step-1-searching-arxiv","title":"Step 1: Searching arXiv","text":"<p>The arXiv API allows searching by title, abstract, author, and category.</p>"},{"location":"pipeline/ingestion/#search-queries","title":"Search Queries","text":"<pre><code>SEARCH_QUERIES = [\n    'ti:\"large language model\" OR abs:\"large language model\"',\n    'ti:\"retrieval augmented generation\" OR abs:\"retrieval augmented generation\"',\n    'ti:transformer OR abs:transformer',\n    'ti:RAG OR abs:RAG',\n]\n</code></pre>"},{"location":"pipeline/ingestion/#category-filtering","title":"Category Filtering","text":"<p>We focus on AI/ML papers:</p> <pre><code>CS_CATEGORIES = {\"cs.AI\", \"cs.CL\", \"cs.LG\", \"cs.IR\", \"cs.NE\"}\n</code></pre> <ul> <li><code>cs.AI</code> - Artificial Intelligence</li> <li><code>cs.CL</code> - Computation and Language (NLP)</li> <li><code>cs.LG</code> - Machine Learning</li> <li><code>cs.IR</code> - Information Retrieval</li> <li><code>cs.NE</code> - Neural and Evolutionary Computing</li> </ul>"},{"location":"pipeline/ingestion/#api-implementation","title":"API Implementation","text":"<pre><code>import arxiv\n\nclient = arxiv.Client(page_size=100, delay_seconds=3, num_retries=3)\nsearch = arxiv.Search(\n    query=query,\n    max_results=100,\n    sort_by=arxiv.SortCriterion.SubmittedDate,\n    sort_order=arxiv.SortOrder.Descending,\n)\n\nfor paper in client.results(search):\n    # Extract metadata\n    paper_data = {\n        \"arxiv_id\": paper.get_short_id(),  # e.g., \"2401.12345\"\n        \"title\": paper.title,\n        \"authors\": [a.name for a in paper.authors],\n        \"abstract\": paper.summary,\n        \"published\": paper.published.isoformat(),\n        \"categories\": paper.categories,\n        \"arxiv_url\": paper.entry_id,\n    }\n</code></pre>"},{"location":"pipeline/ingestion/#rate-limiting","title":"Rate Limiting","text":"<p>arXiv has strict rate limits:</p> <ul> <li>3 second delay between requests</li> <li>Automatic retries on failure</li> <li>Respectful crawling</li> </ul>"},{"location":"pipeline/ingestion/#step-2-storing-metadata","title":"Step 2: Storing Metadata","text":"<p>Papers are stored in SQLite for tracking:</p> <pre><code>CREATE TABLE papers (\n    arxiv_id TEXT PRIMARY KEY,\n    title TEXT,\n    authors TEXT,\n    abstract TEXT,\n    published TEXT,\n    categories TEXT,\n    arxiv_url TEXT,\n    collected_at TEXT,\n    is_indexed INTEGER DEFAULT 0,\n    html_available INTEGER DEFAULT NULL\n);\n</code></pre>"},{"location":"pipeline/ingestion/#deduplication","title":"Deduplication","text":"<p>Papers may appear in multiple search queries. We deduplicate by <code>arxiv_id</code>:</p> <pre><code>def deduplicate_papers(all_papers: list[list[dict]]) -&gt; list[dict]:\n    seen = set()\n    unique = []\n    for papers in all_papers:\n        for paper in papers:\n            if paper[\"arxiv_id\"] not in seen:\n                seen.add(paper[\"arxiv_id\"])\n                unique.append(paper)\n    return unique\n</code></pre>"},{"location":"pipeline/ingestion/#step-3-fetching-html-content","title":"Step 3: Fetching HTML Content","text":""},{"location":"pipeline/ingestion/#why-ar5iv-over-pdfs","title":"Why ar5iv over PDFs?","text":"Aspect PDF ar5iv HTML Text quality Often corrupted Clean Structure Lost Preserved Math Garbled LaTeX Tables Broken Intact <p>ar5iv is a project that renders arXiv papers as HTML, preserving structure.</p>"},{"location":"pipeline/ingestion/#html-fetching","title":"HTML Fetching","text":"<pre><code>from dataclasses import dataclass\nfrom bs4 import BeautifulSoup\nimport httpx\n\n@dataclass\nclass PaperHTML:\n    arxiv_id: str\n    title: str | None\n    abstract: str | None\n    sections: list[Section]\n    success: bool\n    error: str | None = None\n\ndef fetch_paper_html(arxiv_id: str) -&gt; PaperHTML:\n    url = f\"https://ar5iv.labs.arxiv.org/html/{arxiv_id}\"\n    response = httpx.get(url, timeout=30, follow_redirects=True)\n\n    if response.status_code != 200:\n        return PaperHTML(arxiv_id=arxiv_id, success=False, error=\"Not found\")\n\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    # Extract title, abstract, sections...\n</code></pre>"},{"location":"pipeline/ingestion/#section-extraction","title":"Section Extraction","text":"<pre><code>@dataclass\nclass Section:\n    title: str\n    content: str\n\ndef extract_sections(soup: BeautifulSoup) -&gt; list[Section]:\n    sections = []\n    for section in soup.find_all(\"section\"):\n        title = section.find([\"h2\", \"h3\", \"h4\"])\n        title_text = title.get_text(strip=True) if title else \"Untitled\"\n\n        # Get text content, excluding nested sections\n        content = section.get_text(separator=\"\\n\", strip=True)\n        sections.append(Section(title=title_text, content=content))\n\n    return sections\n</code></pre>"},{"location":"pipeline/ingestion/#handling-unavailable-papers","title":"Handling Unavailable Papers","text":"<p>Not all papers have ar5iv HTML (~80% coverage):</p> <pre><code>def fetch_and_chunk_paper(paper: dict) -&gt; dict:\n    paper_html = fetch_paper_html(paper[\"arxiv_id\"])\n\n    if not paper_html.success:\n        # Mark as unavailable, don't retry\n        return {\"status\": \"no_html\", \"chunks\": 0}\n\n    # Continue with chunking...\n</code></pre>"},{"location":"pipeline/ingestion/#step-4-running-the-pipeline","title":"Step 4: Running the Pipeline","text":""},{"location":"pipeline/ingestion/#manual-indexing-api","title":"Manual Indexing (API)","text":"<pre><code>curl -X POST http://localhost:8000/index \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"topic\": \"retrieval augmented generation\", \"max_results\": 10}'\n</code></pre>"},{"location":"pipeline/ingestion/#automated-workflows-prefect","title":"Automated Workflows (Prefect)","text":"<pre><code># Collect papers from arXiv\nuv run python scripts/run_flow.py collect --days 30 --max-per-query 100\n\n# Index collected papers\nuv run python scripts/run_flow.py index --limit 50\n\n# Or run both\nuv run python scripts/run_flow.py full\n</code></pre>"},{"location":"pipeline/ingestion/#manual-indexing-script","title":"Manual Indexing Script","text":"<pre><code># Quick test with specific topic\nuv run python scripts/index_papers.py \\\n  --topic \"large language models\" \\\n  --max-results 10\n</code></pre>"},{"location":"pipeline/ingestion/#pipeline-statistics","title":"Pipeline Statistics","text":"<p>After running the collection pipeline:</p> <pre><code>Collection complete:\n  - queries_run: 7\n  - total_found: 2,847\n  - unique_papers: 1,523\n  - saved_to_db: 1,523\n\nIndexing complete:\n  - processed: 50\n  - successful: 42\n  - no_html: 8\n  - total_chunks: 4,200\n</code></pre>"},{"location":"pipeline/ingestion/#error-handling","title":"Error Handling","text":""},{"location":"pipeline/ingestion/#retry-strategy","title":"Retry Strategy","text":"<pre><code>@task(retries=3, retry_delay_seconds=30)\ndef search_arxiv(query: str, max_results: int):\n    # Automatic retry on network failures\n    ...\n</code></pre>"},{"location":"pipeline/ingestion/#graceful-degradation","title":"Graceful Degradation","text":"<ul> <li>If ar5iv unavailable \u2192 mark paper, don't retry</li> <li>If embedding fails \u2192 log error, continue with others</li> <li>If Qdrant down \u2192 fail fast, alert</li> </ul>"},{"location":"pipeline/ingestion/#best-practices","title":"Best Practices","text":"<ol> <li>Respect rate limits: 3+ seconds between arXiv requests</li> <li>Batch operations: Index in batches of 100 chunks</li> <li>Track state: Use SQLite to avoid re-processing</li> <li>Monitor progress: Prefect UI shows real-time status</li> </ol>"},{"location":"pipeline/retrieval/","title":"Retrieval &amp; Reranking","text":"<p>How we find and rank the most relevant documents for a query.</p>"},{"location":"pipeline/retrieval/#two-stage-retrieval","title":"Two-Stage Retrieval","text":"<p>We use a two-stage approach for optimal speed and precision:</p> <pre><code>flowchart LR\n    Q[Query] --&gt; E[Embed Query]\n    E --&gt; VS[Vector Search&lt;br/&gt;k=20, ~10ms]\n    VS --&gt; RR[Reranker&lt;br/&gt;top 5]\n    RR --&gt; LLM[LLM Generation]\n\n    style VS fill:#e1f5fe\n    style RR fill:#fff3e0</code></pre>"},{"location":"pipeline/retrieval/#why-two-stages","title":"Why Two Stages?","text":"Approach Speed Precision Use Case Vector only Fast Good Large-scale filtering Reranker only Slow Excellent Can't scale Two-stage Balanced Excellent Best of both"},{"location":"pipeline/retrieval/#stage-1-vector-search","title":"Stage 1: Vector Search","text":"<p>Fast approximate nearest neighbor search over the entire corpus.</p>"},{"location":"pipeline/retrieval/#how-it-works","title":"How It Works","text":"<pre><code>def _retrieve_and_rerank(self, question: str, k: int = 5) -&gt; list[dict]:\n    # Fetch more candidates than needed\n    fetch_k = 20 if self.enable_reranking else k\n\n    # Fast vector search\n    results = self.vector_store.search(question, k=fetch_k)\n    # ...\n</code></pre>"},{"location":"pipeline/retrieval/#bi-encoder-architecture","title":"Bi-Encoder Architecture","text":"<pre><code>flowchart LR\n    subgraph \"Bi-Encoder (Fast)\"\n        Q1[Query] --&gt; E1[Encoder]\n        D1[Document] --&gt; E2[Encoder]\n        E1 --&gt; V1[Vector]\n        E2 --&gt; V2[Vector]\n        V1 &amp; V2 --&gt; CS[Cosine Similarity]\n    end</code></pre> <p>Key Property: Document vectors are pre-computed, only query needs encoding at search time.</p>"},{"location":"pipeline/retrieval/#stage-2-reranking-with-jina-reranker-v3","title":"Stage 2: Reranking with jina-reranker-v3","text":"<p>Precise scoring of candidate documents using jina-reranker-v3.</p>"},{"location":"pipeline/retrieval/#why-jina-reranker-v3","title":"Why jina-reranker-v3?","text":"<p>Selected based on MTEB Reranking leaderboard.</p>"},{"location":"pipeline/retrieval/#how-it-works_1","title":"How It Works","text":"<pre><code>from transformers import AutoModel\n\nclass Reranker:\n    def __init__(self, model_name: str = \"jinaai/jina-reranker-v3\"):\n        self.model = AutoModel.from_pretrained(\n            model_name,\n            torch_dtype=\"auto\",\n            trust_remote_code=True,\n        )\n        self.model.eval()\n\n    def rerank(\n        self,\n        query: str,\n        documents: list[dict],\n        top_k: int | None = None,\n    ) -&gt; list[dict]:\n        contents = [doc[\"content\"] for doc in documents]\n        results = self.model.rerank(query, contents, top_n=top_k)\n\n        reranked = []\n        for result in results:\n            doc = documents[result[\"index\"]].copy()\n            doc[\"rerank_score\"] = float(result[\"relevance_score\"])\n            doc[\"original_score\"] = doc.get(\"score\", 0.0)\n            reranked.append(doc)\n\n        return reranked\n</code></pre>"},{"location":"pipeline/retrieval/#configuration","title":"Configuration","text":""},{"location":"pipeline/retrieval/#parameters","title":"Parameters","text":"<pre><code>class RAGChain:\n    def __init__(\n        self,\n        enable_reranking: bool = True,  # Toggle reranking\n        retrieval_k: int = 20,          # Candidates from vector search\n        rerank_top_k: int = 5,          # Final results after reranking\n    ):\n</code></pre>"},{"location":"pipeline/retrieval/#trade-offs","title":"Trade-offs","text":"retrieval_k rerank_top_k Speed Quality 10 3 Fast Lower recall 20 5 Balanced Good 50 10 Slower Better recall"},{"location":"pipeline/retrieval/#score-interpretation","title":"Score Interpretation","text":""},{"location":"pipeline/retrieval/#before-reranking-cosine-similarity","title":"Before Reranking (Cosine Similarity)","text":"<pre><code>Doc 1: 0.78  \u2190 Similar embedding\nDoc 2: 0.75\nDoc 3: 0.72\nDoc 4: 0.71  \u2190 Hard to distinguish\nDoc 5: 0.70\n</code></pre>"},{"location":"pipeline/retrieval/#after-reranking","title":"After Reranking","text":"<pre><code>Doc 3: 0.92  \u2190 Actually most relevant!\nDoc 1: 0.84\nDoc 5: 0.67\nDoc 2: 0.45  \u2190 Reranker found it less relevant\nDoc 4: 0.23\n</code></pre> <p>The reranker often reorders significantly, catching semantic nuances.</p>"},{"location":"pipeline/retrieval/#context-formatting","title":"Context Formatting","text":"<p>Retrieved documents are formatted for the LLM:</p> <pre><code>def _format_context(self, results: list[dict]) -&gt; str:\n    context_parts = []\n    for i, r in enumerate(results, 1):\n        meta = r[\"metadata\"]\n        source_info = f\"[{i}] {meta.get('title', 'Unknown')}\"\n        source_info += f\"\\n    URL: {meta.get('arxiv_url', '')}\"\n        source_info += f\"\\n    Authors: {meta.get('authors', 'Unknown')}\"\n        source_info += f\"\\n    Content: {r['content']}\"\n        context_parts.append(source_info)\n    return \"\\n\\n\".join(context_parts)\n</code></pre>"},{"location":"pipeline/retrieval/#future-improvements","title":"Future Improvements","text":""},{"location":"pipeline/retrieval/#hybrid-search","title":"Hybrid Search","text":"<p>Combine BM25 (keyword) with semantic search:</p> <pre><code># Reciprocal Rank Fusion\ndef hybrid_search(query, alpha=0.5):\n    semantic_results = vector_search(query)\n    keyword_results = bm25_search(query)\n    fused = reciprocal_rank_fusion(semantic_results, keyword_results, alpha)\n    return fused\n</code></pre>"},{"location":"pipeline/retrieval/#query-expansion","title":"Query Expansion","text":"<p>Generate related queries for better recall:</p> <pre><code># Use LLM to expand query\nexpanded = llm.generate(f\"Generate 3 related search queries for: {query}\")\n# Search with all queries, deduplicate results\n</code></pre>"},{"location":"pipeline/vector-storage/","title":"Vector Storage","text":"<p>How embeddings are stored and searched in Qdrant.</p>"},{"location":"pipeline/vector-storage/#qdrant-overview","title":"Qdrant Overview","text":"<p>Qdrant is a vector similarity search engine:</p> <ul> <li>Written in Rust (fast)</li> <li>REST and gRPC APIs</li> <li>Filtering support</li> <li>Persistent storage</li> <li>Easy Docker deployment</li> </ul>"},{"location":"pipeline/vector-storage/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    subgraph Qdrant\n        C1[Collection: papers_qwen3-4b]\n        C2[Collection: papers_qwen3-0.6b]\n        C3[Collection: papers_bge-large]\n    end\n\n    E1[Qwen3-4B Embedder] --&gt; C1\n    E2[Qwen3-0.6B Embedder] --&gt; C2\n    E3[BGE-Large Embedder] --&gt; C3\n\n    Q[Query] --&gt; S{Search}\n    S --&gt; C1\n    S --&gt; C2\n    S --&gt; C3</code></pre> <p>Each embedding model gets its own collection (different dimensions).</p>"},{"location":"pipeline/vector-storage/#collection-setup","title":"Collection Setup","text":"<pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\n\nclass VectorStore:\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = 6333,\n        collection_prefix: str = \"papers\",\n        embedding_model: str = \"qwen3-4b\",\n    ):\n        self.embedder = get_embedder(embedding_model)\n        self.collection_name = f\"{collection_prefix}_{self.embedder.name}\"\n        self.client = QdrantClient(host=host, port=port)\n        self._ensure_collection()\n\n    def _ensure_collection(self) -&gt; None:\n        collections = [c.name for c in self.client.get_collections().collections]\n        if self.collection_name not in collections:\n            self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(\n                    size=self.embedder.dimensions,\n                    distance=Distance.COSINE,\n                ),\n            )\n</code></pre>"},{"location":"pipeline/vector-storage/#indexing-documents","title":"Indexing Documents","text":""},{"location":"pipeline/vector-storage/#point-structure","title":"Point Structure","text":"<p>Each vector is stored as a \"point\":</p> <pre><code>from qdrant_client.models import PointStruct\n\npoint = PointStruct(\n    id=\"md5_hash_of_source_page_chunk\",\n    vector=[0.12, -0.34, ...],  # 2560 floats for qwen3-4b\n    payload={\n        \"content\": \"The transformer architecture...\",\n        \"title\": \"Attention Is All You Need\",\n        \"authors\": \"Vaswani et al.\",\n        \"arxiv_id\": \"1706.03762\",\n        \"arxiv_url\": \"https://arxiv.org/abs/1706.03762\",\n        \"page\": 3,\n        \"chunk\": 0,\n    }\n)\n</code></pre>"},{"location":"pipeline/vector-storage/#batch-upsert","title":"Batch Upsert","text":"<pre><code>def add_documents(self, documents: list[Document]) -&gt; None:\n    texts = [doc.page_content for doc in documents]\n    vectors = self.embedder.encode(texts)\n\n    points = [\n        PointStruct(\n            id=self._generate_id(doc),\n            vector=vec,\n            payload={\"content\": doc.page_content, **doc.metadata},\n        )\n        for doc, vec in zip(documents, vectors)\n    ]\n\n    self.client.upsert(\n        collection_name=self.collection_name,\n        points=points\n    )\n</code></pre>"},{"location":"pipeline/vector-storage/#id-generation","title":"ID Generation","text":"<p>Deterministic IDs prevent duplicates:</p> <pre><code>import hashlib\n\ndef _generate_id(self, doc: Document) -&gt; str:\n    key = f\"{doc.metadata['source']}_{doc.metadata['page']}_{doc.metadata['chunk']}\"\n    return hashlib.md5(key.encode()).hexdigest()\n</code></pre>"},{"location":"pipeline/vector-storage/#searching","title":"Searching","text":""},{"location":"pipeline/vector-storage/#basic-search","title":"Basic Search","text":"<pre><code>def search(self, query: str, k: int = 5) -&gt; list[dict]:\n    query_vector = self.embedder.encode_query(query)\n\n    results = self.client.query_points(\n        collection_name=self.collection_name,\n        query=query_vector,\n        limit=k,\n    ).points\n\n    return [\n        {\n            \"content\": hit.payload.pop(\"content\"),\n            \"metadata\": hit.payload,\n            \"score\": hit.score,\n        }\n        for hit in results\n    ]\n</code></pre>"},{"location":"pipeline/vector-storage/#with-filtering","title":"With Filtering","text":"<pre><code>from qdrant_client.models import Filter, FieldCondition, MatchValue\n\n# Search only papers from 2024\nresults = self.client.query_points(\n    collection_name=self.collection_name,\n    query=query_vector,\n    query_filter=Filter(\n        must=[\n            FieldCondition(\n                key=\"published\",\n                match=MatchValue(value=\"2024\")\n            )\n        ]\n    ),\n    limit=k,\n)\n</code></pre>"},{"location":"pipeline/vector-storage/#filter-options","title":"Filter Options","text":"Filter Example Exact match <code>MatchValue(value=\"cs.AI\")</code> Range <code>Range(gte=0.5, lte=1.0)</code> In list <code>MatchAny(any=[\"cs.AI\", \"cs.CL\"])</code> Text contains <code>MatchText(text=\"transformer\")</code>"},{"location":"pipeline/vector-storage/#hnsw-indexing","title":"HNSW Indexing","text":"<p>Qdrant uses HNSW (Hierarchical Navigable Small World) for fast approximate search:</p> <pre><code>graph TB\n    subgraph \"HNSW Layers\"\n        L2[Layer 2 - Sparse]\n        L1[Layer 1 - Medium]\n        L0[Layer 0 - Dense]\n    end\n\n    Q[Query] --&gt; L2\n    L2 --&gt; L1\n    L1 --&gt; L0\n    L0 --&gt; R[Results]</code></pre>"},{"location":"pipeline/vector-storage/#how-it-works","title":"How It Works","text":"<ol> <li>Start at top layer (sparse, few connections)</li> <li>Find closest neighbors, descend</li> <li>Repeat until bottom layer</li> <li>Return k nearest neighbors</li> </ol>"},{"location":"pipeline/vector-storage/#trade-offs","title":"Trade-offs","text":"Parameter Effect <code>m</code> (connections) Higher = better recall, more memory <code>ef_construct</code> Higher = better index, slower build <code>ef</code> (search) Higher = better recall, slower search <p>Default settings work well for most cases.</p>"},{"location":"pipeline/vector-storage/#storage-statistics","title":"Storage Statistics","text":"<p>Check collection stats:</p> <pre><code>curl http://localhost:6333/collections/papers_qwen3-4b\n</code></pre> <p>Response:</p> <pre><code>{\n  \"result\": {\n    \"status\": \"green\",\n    \"vectors_count\": 45000,\n    \"indexed_vectors_count\": 45000,\n    \"points_count\": 45000,\n    \"segments_count\": 4,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 768,\n          \"distance\": \"Cosine\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"pipeline/vector-storage/#operations","title":"Operations","text":""},{"location":"pipeline/vector-storage/#count-documents","title":"Count Documents","text":"<pre><code>def count(self) -&gt; int:\n    return self.client.count(collection_name=self.collection_name).count\n</code></pre>"},{"location":"pipeline/vector-storage/#clear-collection","title":"Clear Collection","text":"<pre><code>def clear(self) -&gt; None:\n    self.client.delete_collection(collection_name=self.collection_name)\n    self._ensure_collection()\n</code></pre>"},{"location":"pipeline/vector-storage/#delete-specific-points","title":"Delete Specific Points","text":"<pre><code>self.client.delete(\n    collection_name=self.collection_name,\n    points_selector=PointIdsList(\n        points=[\"id1\", \"id2\"],\n    ),\n)\n</code></pre>"},{"location":"pipeline/vector-storage/#performance-tips","title":"Performance Tips","text":""},{"location":"pipeline/vector-storage/#1-batch-operations","title":"1. Batch Operations","text":"<pre><code># Good: batch upsert\nself.client.upsert(collection_name, points=all_points)\n\n# Bad: individual upserts\nfor point in points:\n    self.client.upsert(collection_name, points=[point])\n</code></pre>"},{"location":"pipeline/vector-storage/#2-payload-indexing","title":"2. Payload Indexing","text":"<p>Index frequently filtered fields:</p> <pre><code>self.client.create_payload_index(\n    collection_name=self.collection_name,\n    field_name=\"arxiv_id\",\n    field_schema=\"keyword\"\n)\n</code></pre>"},{"location":"pipeline/vector-storage/#3-quantization","title":"3. Quantization","text":"<p>Reduce memory with quantization:</p> <pre><code>from qdrant_client.models import ScalarQuantization, ScalarType\n\nvectors_config=VectorParams(\n    size=768,\n    distance=Distance.COSINE,\n    quantization_config=ScalarQuantization(\n        type=ScalarType.INT8,\n        always_ram=True\n    )\n)\n</code></pre>"},{"location":"pipeline/vector-storage/#docker-setup","title":"Docker Setup","text":"<pre><code>services:\n  qdrant:\n    image: qdrant/qdrant:latest\n    ports:\n      - \"6333:6333\"  # REST API\n      - \"6334:6334\"  # gRPC\n    volumes:\n      - qdrant_data:/qdrant/storage\n    environment:\n      - QDRANT__SERVICE__GRPC_PORT=6334\n\nvolumes:\n  qdrant_data:\n</code></pre>"},{"location":"pipeline/vector-storage/#monitoring","title":"Monitoring","text":"<p>Qdrant provides a dashboard at <code>http://localhost:6333/dashboard</code>:</p> <ul> <li>Collection statistics</li> <li>Vector visualization</li> <li>Query testing</li> <li>Performance metrics</li> </ul>"}]}